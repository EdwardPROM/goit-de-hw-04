### 1. Перший скріншот (5 Jobs)
- Ми виконуємо ланцюг трансформацій (читаємо CSV, робимо `where`, `groupBy().count()`, потім ще один `where`) і один виклик Action `collect()`.
- Spark виконує обчислення, розбиваючи їх на кілька етапів (Stages), об’єднаних у 5 Jobs.
- Один виклик Action запускає обчислення всього ланцюга.

### 2. Другий скріншот (8 Jobs)
- Замість одного виклику Action тепер маємо два (`.collect()`): проміжний і фінальний.
- Кожен виклик Action змушує Spark виконувати DAG обчислень (або його суттєву частину) вдруге — оскільки немає кешування, Spark читає та обробляє дані спочатку для першого `collect()`, а потім знову для другого.
- У результаті загальна кількість Jobs зростає з 5 до 8.

### 3. Третій скріншот (7 Jobs)
- У цьому варіанті ми додаємо метод `.cache()`, щоб зберегти проміжний результат у пам’яті після першої дії.
- Завдяки цьому, коли викликається другий `collect()`, Spark не виконує всі попередні обчислення заново, а бере дані з кешу, додаючи лише необхідні перетворення.
- Тому кількість Jobs зменшується порівняно з другим варіантом — тепер їх 7 замість 8.